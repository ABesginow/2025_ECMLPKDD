{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0eb78",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from LODEGP.LODEGP import LODEGP, list_standard_models\n",
    "import gpytorch\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from finite_difference import central_difference\n",
    "from helpers.training_functions import granso_optimization\n",
    "from helpers.util_functions import get_full_kernels_in_kernel_expression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72dcdc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c4988",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Base ODE:     \n",
    "# [x**2 + 981/(100*l1), 0, -1/l1, 0, x**2+981/(100*l2), -1/l2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a109a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "START = 2r\n",
    "END = 12r\n",
    "COUNT = 100r\n",
    "train_x = torch.linspace(START, END, COUNT)\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=5r)\n",
    "\n",
    "y0_func = lambda x: float(781/8000)*torch.sin(x)/x - float(1/20)*torch.cos(x)/x**2 + float(1/20)*torch.sin(x)/x**3\n",
    "y1_func = lambda x: float(881/8000)*torch.sin(x)/x - float(1/40)*torch.cos(x)/x**2 + float(1/40)*torch.sin(x)/x**3\n",
    "y2_func = lambda x: float(688061/800000)*torch.sin(x)/x - float(2543/4000)*torch.cos(x)/x**2 + float(1743/4000)*torch.sin(x)/x**3 - float(3/5)*torch.cos(x)/x**4 + float(3/5)*torch.sin(x)/x**5 \n",
    "y0 = y0_func(train_x)\n",
    "y1 = y1_func(train_x)\n",
    "y2 = y2_func(train_x)\n",
    "train_y = torch.stack([y0, y1, y2], dim=-1r)\n",
    "\n",
    "model = LODEGP(train_x, train_y, likelihood, 3, ODE_name=\"Bipendulum\", verbose=True, system_parameters={\"l1\": 1.0, \"l2\": 2.0})\n",
    "#model = LODEGP(train_x, train_y, likelihood, 5, ODE_name=\"Heating\", verbose=True, system_parameters={\"l1\": 1.0, \"l2\": 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871ff16",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model.matrix_multiplication\n",
    "# This should satisfy the ODEs per column?\n",
    "model.diffed_kernel \n",
    "model.sum_diff_replaced[2][0]\n",
    "#str(model.covar_description[0][0])[47:-10]\n",
    "#model.covar_description\n",
    "#model.covar_module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc257b8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "var(\"signal_variance_1, t_diff, lengthscale_1, signal_variance_2, lengthscale_2, t1, t2\") \n",
    "\n",
    "k_00 = 962361/40000*signal_variance_2^2*e^(-0.5*t_diff^2/lengthscale_2^2) - 9.81*signal_variance_2^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^2 + 9.81*signal_variance_2^2*t_diff^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^4 + 3.0*signal_variance_2^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^4 - 6.0*signal_variance_2^2*t_diff^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^6 + 1.0*signal_variance_2^2*t_diff^4*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^8\n",
    "k_10 = 962361/40000*signal_variance_2^2*e^(-0.5*t_diff^2/lengthscale_2^2) - 7.3575*signal_variance_2^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^2 + 7.3575*signal_variance_2^2*t_diff^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^4 + 1.5*signal_variance_2^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^4 - 3.0*signal_variance_2^2*t_diff^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^6 + 0.5*signal_variance_2^2*t_diff^4*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^8\n",
    "k_20 = 944076141/4000000*signal_variance_2^2*e^(-0.5*t_diff^2/lengthscale_2^2) - 120.295125*signal_variance_2^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^2 + 120.295125*signal_variance_2^2*t_diff^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^4 + 58.86*signal_variance_2^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^4 - 117.72*signal_variance_2^2*t_diff^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^6 + 19.62*signal_variance_2^2*t_diff^4*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^8 - 15.0*signal_variance_2^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^6 + 45.0*signal_variance_2^2*t_diff^2*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^8 - 15.0*signal_variance_2^2*t_diff^4*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^10 + 1.0*signal_variance_2^2*t_diff^6*e^(-0.5*t_diff^2/lengthscale_2^2)/lengthscale_2^12\n",
    "k_00 = k_00.substitute(t_diff==t1-t2)\n",
    "k_10 = k_10.substitute(t_diff==t1-t2)\n",
    "k_20 = k_20.substitute(t_diff==t1-t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4a1d1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#print((f0.diff(t, 2) + 9.81*f0 - f2))\n",
    "#print((f1.diff(t, 2) + 9.81/2*f1 - f2/2))\n",
    "(k_00.diff(t1, 2) + 9.81*k_00 - k_20)\n",
    "k_10.diff(t1, 2) + 9.81/2*k_10 - k_20/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542758b9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# If I want to extract the string of the function\n",
    "#str(model.covar_description[0][0])[47:-10]\n",
    "\n",
    "def covar_description_function00(x1, x2, row_pos=0, col_pos=0):\n",
    "    common_terms = {\n",
    "                \"t_diff\" : x1-x2.t(),\n",
    "                \"t_sum\" : x1+x2.t(),\n",
    "                \"t_ones\": torch.ones_like(x1-x2.t()),\n",
    "                \"t_zeroes\": torch.zeros_like(x1-x2.t())\n",
    "            }\n",
    "    model_parameters = model.model_parameters\n",
    "    return eval(model.covar_description[row_pos][col_pos]).detach().numpy()\n",
    "\n",
    "\n",
    "h = 1e-4r\n",
    "target_column = 2\n",
    "for value in torch.linspace(0.0r, 1.0r, 1r):\n",
    "    print(value)\n",
    "    #print((f0.diff(t, 2) + 9.81*f0 - f2))\n",
    "    #print((central_difference(covar_description_function00, value, h=1e-1r, precision=6, order=2) + 9.81*covar_description_function00(value, 0, 0) - covar_description_function00(value, 2, 0)))\n",
    "    print((covar_description_function00(value+h, value, 0, target_column) - 2 * covar_description_function00(value, value, 0, target_column) + covar_description_function00(value-h, value, 0, target_column))/(h**2) + 9.81*covar_description_function00(value, value, 0, target_column) -  covar_description_function00(value, value, 2, target_column))\n",
    "    \n",
    "\n",
    "\n",
    "    #print((f1.diff(t, 2) + 9.81/2*f1 - f2/2))\n",
    "    print((covar_description_function00(value+h, value, 1, target_column) - 2 * covar_description_function00(value, value, 1, target_column) + covar_description_function00(value-h, value, 1, target_column))/(h**2) + 9.81/2*covar_description_function00(value, value, 1, target_column) -  covar_description_function00(value, value, 2, target_column)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfab30a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Comparison whether the central difference of the eval kernel is to the computer algebra derivative\n",
    "\n",
    "#print((f0.diff(t, 2) + 9.81*f0 - f2))\n",
    "#print((f1.diff(t, 2) + 9.81/2*f1 - f2/2))\n",
    "var(\"t1, t2\")\n",
    "\n",
    "row_pos = 0\n",
    "col_pos = 0\n",
    "\n",
    "\n",
    "def covar_description_function00(x1, x2, row_pos=0, col_pos=0):\n",
    "    common_terms = {\n",
    "                \"t_diff\" : x1-x2.t(),\n",
    "                \"t_sum\" : x1+x2.t(),\n",
    "                \"t_ones\": torch.ones_like(x1-x2.t()),\n",
    "                \"t_zeroes\": torch.zeros_like(x1-x2.t())\n",
    "            }\n",
    "    model_parameters = model.model_parameters\n",
    "    return eval(model.covar_description[row_pos][col_pos]).detach().numpy()\n",
    "\n",
    "h = 1e-4r \n",
    "\n",
    "for value in torch.linspace(0.0r, 1.0r, 1r):\n",
    "    compalg_val = model.diffed_kernel[row_pos][col_pos].diff(t1, 2)(t1=float(value), t2=float(value), signal_variance_2=1.0, lengthscale_2=1.0)\n",
    "    fin_diff_torch_val = (covar_description_function00(value+h, value) - 2 * covar_description_function00(value, value) + covar_description_function00(value-h, value))/(h**2)\n",
    "    print(f\"compalg_val: {compalg_val}, fin_diff_torch_val: {fin_diff_torch_val}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860c2ee",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model.covar_description[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2e19b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Comparison whether the computer algebra function does the same as the torch eval function\n",
    "#print((f0.diff(t, 2) + 9.81*f0 - f2))\n",
    "#print((f1.diff(t, 2) + 9.81/2*f1 - f2/2))\n",
    "var(\"t1, t2\")\n",
    "\n",
    "\n",
    "row_pos = 0\n",
    "col_pos = 0\n",
    "for value in torch.linspace(0.0r, 1.0r, 1r):\n",
    "    compalg_val = model.diffed_kernel[row_pos][col_pos](t1=float(value), t2=float(value), signal_variance_2=1.0, lengthscale_2=1.0)\n",
    "    fin_diff_torch_val = covar_description_function00(value, value, row_pos, col_pos)\n",
    "    print(f\"compalg_val: {compalg_val}, fin_diff_torch_val: {fin_diff_torch_val}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4df2c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Column wise satisfaction of the ODEs through sagemath entries within the LODEGP \n",
    "target_col = 0\n",
    "diffed_kernel_column = [model.diffed_kernel[row][target_col] for row in range(model.num_tasks)]\n",
    "\n",
    "#print((f0.diff(t, 2) + 9.81*f0 - f2))\n",
    "#print((f1.diff(t, 2) + 9.81/2*f1 - f2/2))\n",
    "var(\"t1\")\n",
    "\n",
    "print((diffed_kernel_column[0].diff(t1, 2) + 9.81*diffed_kernel_column[0] - diffed_kernel_column[2])(t1=1, t2=1, signal_variance_2=1.0, lengthscale_2=1.0))\n",
    "(diffed_kernel_column[1].diff(t1, 2) + 9.81/2*diffed_kernel_column[1] - diffed_kernel_column[2]/2)(t1=1, t2=1, signal_variance_2=1.0, lengthscale_2=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86d04b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "V = model.V\n",
    "var(\"t\")\n",
    "base_func = 1/(40*t)*sin(t)\n",
    "p = matrix([[0],[0],[base_func]])\n",
    "f = V*p\n",
    "print(f)\n",
    "\n",
    "f0 = 1/200*(200*base_func.diff(t, 2) + 981*base_func)\n",
    "f1 = 1/200*(100*base_func.diff(t, 2) + 981*base_func)\n",
    "f2 = 1/20000*(20000*base_func.diff(t, 4) + 294300*base_func.diff(t, 2) + 981**2*base_func)\n",
    "plot(f1, (t, -10, 10), color='red')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c3355",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Check if both ODEs are satisfied\n",
    "print((f0.diff(t, 2) + 9.81*f0 - f2))\n",
    "print((f1.diff(t, 2) + 9.81/2*f1 - f2/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36dd75",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571484cb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "values = list()\n",
    "differences = list()\n",
    "for value in train_x:\n",
    "    cd_val = float(central_difference(y0_func, value, h=1e-1r, precision=6, order=2))\n",
    "    compalg_val = f0.diff(t, 2)(t=float(value)).n()\n",
    "    values.append((cd_val, compalg_val))\n",
    "    differences.append(abs(cd_val - compalg_val))\n",
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7fbf3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# ODE 1\n",
    "#(f0.diff(t, 2) + 9.81*f0 - f2)\n",
    "h = 1e-1r\n",
    "print(\"ODE1 median error: \\t\" , np.median(np.abs(central_difference(y0_func, train_x, h=h, precision=6, order=2) + 9.81r*y0_func(train_x) - y2_func(train_x))))\n",
    "print(\"ODE1 avg. error: \\t\" , np.average(np.abs(central_difference(y0_func, train_x, h=h, precision=6, order=2) + 9.81r*y0_func(train_x) - y2_func(train_x))))\n",
    "\n",
    "\n",
    "\n",
    "# ODE 2\n",
    "#(f1.diff(t, 2) + 9.81/2*f1 - f2/2)\n",
    "\n",
    "print(\"ODE2 median error: \\t\" , np.median(np.abs(central_difference(y1_func, train_x, h=h, precision=6, order=2) + 9.81r*y1_func(train_x)*0.5r - y2_func(train_x)*0.5r)))\n",
    "print(\"ODE2 avg. error: \\t\" , np.average(np.abs(central_difference(y1_func, train_x, h=h, precision=6, order=2) + 9.81r*y1_func(train_x)*0.5r - y2_func(train_x)*0.5r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d74bea",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "fdd028e6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Adam training loop for the model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1r)  # Includes GaussianLikelihood parameters\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "training_iter = 1000\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()  # Zero gradients from previous iteration\n",
    "    output = model(train_x)  # Output from model\n",
    "    loss = -mll(output, train_y)  # Negative log marginal likelihood\n",
    "    loss.backward()  # Derivatives into optimizer\n",
    "    optimizer.step()  # Update parameters\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iter {i + 1}/{training_iter} - Loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a59b5f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the posterior GP and the data\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    test_x = torch.linspace(2r, 12r, 100r)\n",
    "    observed_pred = model.likelihood(model(test_x))\n",
    "    observed_pred_mean = observed_pred.mean\n",
    "    observed_pred_var = observed_pred.covariance_matrix.diag().reshape(-1, 3)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    for i in range( 3):\n",
    "        ax.plot(test_x.numpy(), observed_pred_mean[:, i].numpy(), 'k')\n",
    "        ax.fill_between(test_x.numpy(),\n",
    "                        observed_pred_mean[:, i].numpy() - 1.96 * observed_pred_var[:, i].sqrt().numpy(),\n",
    "                        observed_pred_mean[:, i].numpy() + 1.96 * observed_pred_var[:, i].sqrt().numpy(),\n",
    "                        alpha=0.5)\n",
    "        ax.scatter(train_x.numpy(), train_y[:, i].numpy(), s=10, c='r', marker='x')\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title('Posterior GP')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca60a8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_param_spec(\n",
    "    param_name,\n",
    "    kernel_name,\n",
    "    param_specs=None,\n",
    "    kernel_param_specs=None,\n",
    "    default_spec=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Resolves a parameter spec using full name match, kernel+param, or fuzzy match for LODE_Kernel.\n",
    "\n",
    "    Returns:\n",
    "    - spec dict (e.g., {\"bounds\": (a, b), \"type\": ..., \"mean\": ..., \"variance\": ...})\n",
    "    \"\"\"\n",
    "    param_specs = param_specs or {}\n",
    "    kernel_param_specs = kernel_param_specs or {}\n",
    "    default_spec = default_spec or {}\n",
    "\n",
    "    # 1. Full name override\n",
    "    if param_name in param_specs:\n",
    "        return param_specs[param_name]\n",
    "\n",
    "    # 2. Kernel + parameter\n",
    "    if (kernel_name, param_name) in kernel_param_specs:\n",
    "        return kernel_param_specs[(kernel_name, param_name)]\n",
    "\n",
    "    # 3. Fuzzy base-key match for LODE_Kernel\n",
    "    if kernel_name == \"LODE_Kernel\":\n",
    "        for base_key in [\"lengthscale\", \"signal_variance\"]:\n",
    "            if base_key in param_name:\n",
    "                if (kernel_name, base_key) in kernel_param_specs:\n",
    "                    return kernel_param_specs[(kernel_name, base_key)]\n",
    "\n",
    "    # 4. Fallback\n",
    "    return default_spec\n",
    "\n",
    "def collect_hyperparameter_prior_specs(\n",
    "    model,\n",
    "    param_specs=None,\n",
    "    kernel_param_specs=None,\n",
    "    default_mean=0.0,\n",
    "    default_variance=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Collects prior parameters (mean, variance) for all named hyperparameters in the model.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples: (param_name, tensor, mean, variance)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    kernel_types = get_full_kernels_in_kernel_expression(model.covar_module)\n",
    "    kernel_index = 0\n",
    "\n",
    "    for name, param in model.named_hyperparameters():\n",
    "        param_tensor = param\n",
    "        local_param_name = name.split(\".\")[-1]\n",
    "\n",
    "        if name in param_specs:\n",
    "            kernel_type = \"<explicit>\"\n",
    "        elif \"LODE_Kernel\" in kernel_types and \"covar_module\" in name:\n",
    "            kernel_type = \"LODE_Kernel\"\n",
    "        else:\n",
    "            kernel_type = kernel_types[kernel_index] if kernel_index < len(kernel_types) else \"<unknown>\"\n",
    "            kernel_index += 1\n",
    "\n",
    "        spec = get_param_spec(\n",
    "            name,\n",
    "            kernel_type,\n",
    "            param_specs=param_specs,\n",
    "            kernel_param_specs=kernel_param_specs,\n",
    "            default_spec={\"mean\": default_mean, \"variance\": default_variance}\n",
    "        )\n",
    "\n",
    "        results.append((name, param_tensor, spec[\"mean\"], spec[\"variance\"]))\n",
    "\n",
    "    return results\n",
    "\n",
    "kernel_param_specs = {\n",
    "    (\"RBFKernel\", \"lengthscale\"): {\"mean\": 0.0, \"variance\": 0.5},\n",
    "    (\"MaternKernel\", \"lengthscale\"): {\"mean\": 1.0, \"variance\": 2.0},\n",
    "    (\"ScaleKernel\", \"outputscale\"): {\"mean\": 0.5, \"variance\": 1.0},\n",
    "    (\"LODE_Kernel\", \"signal_variance\"): {\"mean\": 0.1, \"variance\": 0.1}\n",
    "}\n",
    "\n",
    "param_specs = {\n",
    "    \"likelihood.noise\": {\"mean\": -2.0, \"variance\": 0.25}\n",
    "}\n",
    "\n",
    "model = LODEGP(train_x, train_y, likelihood, 3, ODE_name=\"Bipendulum Parameterized\", verbose=True, system_parameters={\"l1\": 1.0, \"l2\": 2.0})\n",
    "\n",
    "prior_specs = collect_hyperparameter_prior_specs(\n",
    "    model,\n",
    "    param_specs=param_specs,\n",
    "    kernel_param_specs=kernel_param_specs,\n",
    "    default_mean=0.0,\n",
    "    default_variance=1.0\n",
    ")\n",
    "print(prior_specs)\n",
    "mean_list = [p_spec[2] for p_spec in prior_specs]\n",
    "mean_list\n",
    "var_list = [p_spec[3] for p_spec in prior_specs]\n",
    "var_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.5",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "sage",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
