{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LODEGP.LODEGP import LODEGP, list_standard_models\n",
    "import gpytorch\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from finite_difference import central_difference\n",
    "from helpers.training_functions import granso_optimization\n",
    "from helpers.util_functions import get_full_kernels_in_kernel_expression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49204e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = matrix([[                                0,                          -400/981 ,                    x^2 + 981/200]\n",
    ",[                                0                  ,        -200/981             ,    1/2*x^2 + 981/200]\n",
    ",[                               -1                  ,-400/981*x^2 - 4 ,x^4 + 2943/200*x^2 + 962361/20000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base ODE:     \n",
    "# [x**2 + 981/(100*l1), 0, -1/l1, 0, x**2+981/(100*l2), -1/l2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "var(\"t\")\n",
    "base_func = 1/(40*t)*sin(t)\n",
    "p = matrix([[0],[0],[base_func]])\n",
    "f = V*p\n",
    "print(f)\n",
    "\n",
    "f0 = 1/200*(200*base_func.diff(t, 2) + 981*base_func)\n",
    "f1 = 1/200*(100*base_func.diff(t, 2) + 981*base_func)\n",
    "f2 = 1/20000*(20000*base_func.diff(t, 4) + 294300*base_func.diff(t, 2) + 981**2*base_func)\n",
    "plot(f1, (t, -10, 10), color='red')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if both ODEs are satisfied\n",
    "print((f0.diff(t, 2) + 9.81*f0 - f2))\n",
    "print((f1.diff(t, 2) + 9.81/2*f1 - f2/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 2r\n",
    "END = 12r\n",
    "COUNT = 100r\n",
    "train_x = torch.linspace(START, END, COUNT)\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=5r)\n",
    "\n",
    "y0_func = lambda x: float(781/8000)*torch.sin(x)/x - float(1/20)*torch.cos(x)/x**2 + float(1/20)*torch.sin(x)/x**3\n",
    "y1_func = lambda x: float(881/8000)*torch.sin(x)/x - float(1/40)*torch.cos(x)/x**2 + float(1/40)*torch.sin(x)/x**3\n",
    "y2_func = lambda x: float(688061/800000)*torch.sin(x)/x - float(2543/4000)*torch.cos(x)/x**2 + float(1743/4000)*torch.sin(x)/x**3 - float(3/5)*torch.cos(x)/x**4 + float(3/5)*torch.sin(x)/x**5 \n",
    "y0 = y0_func(train_x)\n",
    "y1 = y1_func(train_x)\n",
    "y2 = y2_func(train_x)\n",
    "train_y = torch.stack([y0, y1, y2], dim=-1r)\n",
    "\n",
    "model = LODEGP(train_x, train_y, likelihood, 3, ODE_name=\"Bipendulum\", verbose=True, system_parameters={\"l1\": 1.0, \"l2\": 2.0})\n",
    "#model = LODEGP(train_x, train_y, likelihood, 5, ODE_name=\"Heating\", verbose=True, system_parameters={\"l1\": 1.0, \"l2\": 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571484cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list()\n",
    "differences = list()\n",
    "for value in train_x:\n",
    "    cd_val = float(central_difference(y0_func, value, h=1e-1r, precision=6, order=1))\n",
    "    compalg_val = f0.diff(t, 1)(t=float(value)).n()\n",
    "    values.append((cd_val, compalg_val))\n",
    "    differences.append(abs(cd_val - compalg_val))\n",
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODE 1\n",
    "#(f0.diff(t, 2) + 9.81*f0 - f2)\n",
    "h = 1e-1r\n",
    "print(\"ODE1 median error: \\t\" , np.median(np.abs(central_difference(y0_func, train_x, h=h, precision=6, order=2) + 9.81r*y0_func(train_x) - y2_func(train_x))))\n",
    "print(\"ODE1 avg. error: \\t\" , np.average(np.abs(central_difference(y0_func, train_x, h=h, precision=6, order=2) + 9.81r*y0_func(train_x) - y2_func(train_x))))\n",
    "\n",
    "\n",
    "\n",
    "# ODE 2\n",
    "#(f1.diff(t, 2) + 9.81/2*f1 - f2/2)\n",
    "\n",
    "print(\"ODE2 median error: \\t\" , np.median(np.abs(central_difference(y1_func, train_x, h=h, precision=6, order=2) + 9.81r*y1_func(train_x)*0.5r - y2_func(train_x)*0.5r)))\n",
    "print(\"ODE2 avg. error: \\t\" , np.average(np.abs(central_difference(y1_func, train_x, h=h, precision=6, order=2) + 9.81r*y1_func(train_x)*0.5r - y2_func(train_x)*0.5r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d74bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "fdd028e6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Adam training loop for the model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1r)  # Includes GaussianLikelihood parameters\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "training_iter = 1000\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()  # Zero gradients from previous iteration\n",
    "    output = model(train_x)  # Output from model\n",
    "    loss = -mll(output, train_y)  # Negative log marginal likelihood\n",
    "    loss.backward()  # Derivatives into optimizer\n",
    "    optimizer.step()  # Update parameters\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iter {i + 1}/{training_iter} - Loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a59b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior GP and the data\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    test_x = torch.linspace(2r, 12r, 100r)\n",
    "    observed_pred = model.likelihood(model(test_x))\n",
    "    observed_pred_mean = observed_pred.mean\n",
    "    observed_pred_var = observed_pred.covariance_matrix.diag().reshape(-1, 3)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    for i in range( 3):\n",
    "        ax.plot(test_x.numpy(), observed_pred_mean[:, i].numpy(), 'k')\n",
    "        ax.fill_between(test_x.numpy(),\n",
    "                        observed_pred_mean[:, i].numpy() - 1.96 * observed_pred_var[:, i].sqrt().numpy(),\n",
    "                        observed_pred_mean[:, i].numpy() + 1.96 * observed_pred_var[:, i].sqrt().numpy(),\n",
    "                        alpha=0.5)\n",
    "        ax.scatter(train_x.numpy(), train_y[:, i].numpy(), s=10, c='r', marker='x')\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title('Posterior GP')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca60a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_spec(\n",
    "    param_name,\n",
    "    kernel_name,\n",
    "    param_specs=None,\n",
    "    kernel_param_specs=None,\n",
    "    default_spec=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Resolves a parameter spec using full name match, kernel+param, or fuzzy match for LODE_Kernel.\n",
    "\n",
    "    Returns:\n",
    "    - spec dict (e.g., {\"bounds\": (a, b), \"type\": ..., \"mean\": ..., \"variance\": ...})\n",
    "    \"\"\"\n",
    "    param_specs = param_specs or {}\n",
    "    kernel_param_specs = kernel_param_specs or {}\n",
    "    default_spec = default_spec or {}\n",
    "\n",
    "    # 1. Full name override\n",
    "    if param_name in param_specs:\n",
    "        return param_specs[param_name]\n",
    "\n",
    "    # 2. Kernel + parameter\n",
    "    if (kernel_name, param_name) in kernel_param_specs:\n",
    "        return kernel_param_specs[(kernel_name, param_name)]\n",
    "\n",
    "    # 3. Fuzzy base-key match for LODE_Kernel\n",
    "    if kernel_name == \"LODE_Kernel\":\n",
    "        for base_key in [\"lengthscale\", \"signal_variance\"]:\n",
    "            if base_key in param_name:\n",
    "                if (kernel_name, base_key) in kernel_param_specs:\n",
    "                    return kernel_param_specs[(kernel_name, base_key)]\n",
    "\n",
    "    # 4. Fallback\n",
    "    return default_spec\n",
    "\n",
    "\n",
    "def collect_hyperparameter_prior_specs(\n",
    "    model,\n",
    "    param_specs=None,\n",
    "    kernel_param_specs=None,\n",
    "    default_mean=0.0,\n",
    "    default_variance=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Collects prior parameters (mean, variance) for all named hyperparameters in the model.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples: (param_name, tensor, mean, variance)\n",
    "    \"\"\"\n",
    "    mean_values = []\n",
    "    variance_values = []\n",
    "    kernel_types = get_full_kernels_in_kernel_expression(model.covar_module)\n",
    "    kernel_index = 0\n",
    "\n",
    "    for name, param in model.named_hyperparameters():\n",
    "        param_tensor = param\n",
    "        local_param_name = name.split(\".\")[-1]\n",
    "\n",
    "        if name in param_specs:\n",
    "            kernel_type = \"<explicit>\"\n",
    "        elif \"LODE_Kernel\" in kernel_types:\n",
    "            kernel_type = \"LODE_Kernel\"\n",
    "        else:\n",
    "            kernel_type = kernel_types[kernel_index] if kernel_index < len(kernel_types) else \"<unknown>\"\n",
    "            kernel_index += 1\n",
    "\n",
    "        spec = get_param_spec(\n",
    "            name,\n",
    "            kernel_type,\n",
    "            param_specs=param_specs,\n",
    "            kernel_param_specs=kernel_param_specs,\n",
    "            default_spec={\"mean\": default_mean, \"variance\": default_variance}\n",
    "        )\n",
    "\n",
    "        mean_values.append(spec[\"mean\"])\n",
    "        variance_values.append(spec[\"variance\"])\n",
    "\n",
    "    return mean_values, variance_values\n",
    "\n",
    "kernel_param_specs = {\n",
    "    (\"RBFKernel\", \"lengthscale\"): {\"mean\": 0.0, \"variance\": 0.5},\n",
    "    (\"MaternKernel\", \"lengthscale\"): {\"mean\": 1.0, \"variance\": 2.0},\n",
    "    (\"ScaleKernel\", \"outputscale\"): {\"mean\": 0.5, \"variance\": 1.0},\n",
    "    (\"LODE_Kernel\", \"signal_variance\"): {\"mean\": 0.1, \"variance\": 0.1}\n",
    "}\n",
    "\n",
    "param_specs = {\n",
    "    \"likelihood.noise\": {\"mean\": -2.0, \"variance\": 0.25}\n",
    "}\n",
    "\n",
    "model = LODEGP(train_x, train_y, likelihood, 3, ODE_name=\"Bipendulum Parameterized\", verbose=True, system_parameters={\"l1\": 1.0, \"l2\": 2.0})\n",
    "\n",
    "prior_specs = collect_hyperparameter_prior_specs(\n",
    "    model,\n",
    "    param_specs=param_specs,\n",
    "    kernel_param_specs=kernel_param_specs,\n",
    "    default_mean=0.0,\n",
    "    default_variance=1.0\n",
    ")\n",
    "\n",
    "mean_list = [p_spec[2] for p_spec in prior_specs]\n",
    "mean_list\n",
    "var_list = [p_spec[3] for p_spec in prior_specs]\n",
    "var_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.5",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
