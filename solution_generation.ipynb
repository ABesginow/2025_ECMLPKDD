{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LODEGP.LODEGP import LODEGP, list_standard_models\n",
    "import gpytorch\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from finite_difference import central_difference\n",
    "from helpers.training_functions import granso_optimization\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49204e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = matrix([[                                0,                          -400/981 ,                    x^2 + 981/200]\n",
    ",[                                0                  ,        -200/981             ,    1/2*x^2 + 981/200]\n",
    ",[                               -1                  ,-400/981*x^2 - 4 ,x^4 + 2943/200*x^2 + 962361/20000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base ODE:     \n",
    "# [x**2 + 981/(100*l1), 0, -1/l1, 0, x**2+981/(100*l2), -1/l2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "var(\"t\")\n",
    "base_func = 1/(40*t)*sin(t)\n",
    "p = matrix([[0],[0],[base_func]])\n",
    "f = V*p\n",
    "print(f)\n",
    "\n",
    "f0 = 1/200*(200*base_func.diff(t, 2) + 981*base_func)\n",
    "f1 = 1/200*(100*base_func.diff(t, 2) + 981*base_func)\n",
    "f2 = 1/20000*(20000*base_func.diff(t, 4) + 294300*base_func.diff(t, 2) + 981**2*base_func)\n",
    "plot(f1, (t, -10, 10), color='red')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if both ODEs are satisfied\n",
    "print((f0.diff(t, 2) + 9.81*f0 - f2))\n",
    "print((f1.diff(t, 2) + 9.81/2*f1 - f2/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 2r\n",
    "END = 12r\n",
    "COUNT = 100r\n",
    "train_x = torch.linspace(START, END, COUNT)\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=3r)\n",
    "\n",
    "y0_func = lambda x: float(781/8000)*torch.sin(x)/x - float(1/20)*torch.cos(x)/x**2 + float(1/20)*torch.sin(x)/x**3\n",
    "y1_func = lambda x: float(881/8000)*torch.sin(x)/x - float(1/40)*torch.cos(x)/x**2 + float(1/40)*torch.sin(x)/x**3\n",
    "y2_func = lambda x: float(688061/800000)*torch.sin(x)/x - float(2543/4000)*torch.cos(x)/x**2 + float(1743/4000)*torch.sin(x)/x**3 - float(3/5)*torch.cos(x)/x**4 + float(3/5)*torch.sin(x)/x**5 \n",
    "y0 = y0_func(train_x)\n",
    "y1 = y1_func(train_x)\n",
    "y2 = y2_func(train_x)\n",
    "train_y = torch.stack([y0, y1, y2], dim=-1r)\n",
    "\n",
    "model = LODEGP(train_x, train_y, likelihood, 3, ODE_name=\"Bipendulum\", verbose=True, system_parameters={\"l1\": 1.0, \"l2\": 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571484cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list()\n",
    "differences = list()\n",
    "for value in train_x:\n",
    "    cd_val = float(central_difference(y0_func, value, h=1e-1r, precision=6, order=1))\n",
    "    compalg_val = f0.diff(t, 1)(t=float(value)).n()\n",
    "    values.append((cd_val, compalg_val))\n",
    "    differences.append(abs(cd_val - compalg_val))\n",
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODE 1\n",
    "#(f0.diff(t, 2) + 9.81*f0 - f2)\n",
    "h = 1e-1r\n",
    "print(\"ODE1 median error: \\t\" , np.median(np.abs(central_difference(y0_func, train_x, h=h, precision=6, order=2) + 9.81r*y0_func(train_x) - y2_func(train_x))))\n",
    "print(\"ODE1 avg. error: \\t\" , np.average(np.abs(central_difference(y0_func, train_x, h=h, precision=6, order=2) + 9.81r*y0_func(train_x) - y2_func(train_x))))\n",
    "\n",
    "\n",
    "\n",
    "# ODE 2\n",
    "#(f1.diff(t, 2) + 9.81/2*f1 - f2/2)\n",
    "\n",
    "print(\"ODE2 median error: \\t\" , np.median(np.abs(central_difference(y1_func, train_x, h=h, precision=6, order=2) + 9.81r*y1_func(train_x)*0.5r - y2_func(train_x)*0.5r)))\n",
    "print(\"ODE2 avg. error: \\t\" , np.average(np.abs(central_difference(y1_func, train_x, h=h, precision=6, order=2) + 9.81r*y1_func(train_x)*0.5r - y2_func(train_x)*0.5r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d74bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "fdd028e6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Adam training loop for the model\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1r)  # Includes GaussianLikelihood parameters\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "training_iter = 1000\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()  # Zero gradients from previous iteration\n",
    "    output = model(train_x)  # Output from model\n",
    "    loss = -mll(output, train_y)  # Negative log marginal likelihood\n",
    "    loss.backward()  # Derivatives into optimizer\n",
    "    optimizer.step()  # Update parameters\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iter {i + 1}/{training_iter} - Loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a59b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior GP and the data\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    test_x = torch.linspace(2r, 12r, 100r)\n",
    "    observed_pred = model.likelihood(model(test_x))\n",
    "    observed_pred_mean = observed_pred.mean\n",
    "    observed_pred_var = observed_pred.covariance_matrix.diag().reshape(-1, 3)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    for i in range( 3):\n",
    "        ax.plot(test_x.numpy(), observed_pred_mean[:, i].numpy(), 'k')\n",
    "        ax.fill_between(test_x.numpy(),\n",
    "                        observed_pred_mean[:, i].numpy() - 1.96 * observed_pred_var[:, i].sqrt().numpy(),\n",
    "                        observed_pred_mean[:, i].numpy() + 1.96 * observed_pred_var[:, i].sqrt().numpy(),\n",
    "                        alpha=0.5)\n",
    "        ax.scatter(train_x.numpy(), train_y[:, i].numpy(), s=10, c='r', marker='x')\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title('Posterior GP')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca60a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.util_functions import get_full_kernels_in_kernel_expression\n",
    "\n",
    "#for param in model.state_dict():\n",
    "#    print(param)\n",
    "#    print(model.state_dict()[param])\n",
    "#list(model.named_hyperparameters())\n",
    "list(model.covar_module.named_children())\n",
    "k = gpytorch.kernels.RBFKernel() + gpytorch.kernels.RBFKernel()\n",
    "for name, sub_kernel in k.named_children():\n",
    "    print(name)\n",
    "    print(sub_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f136de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def randomize_kernel_hyperparameters(kernel, bounds):\n",
    "    \"\"\"\n",
    "    Recursively reinitialize hyperparameters of a GP kernel module.\n",
    "\n",
    "    Parameters:\n",
    "    - kernel: a GPyTorch kernel module (may be composed)\n",
    "    - bounds: dict mapping kernel class names to dicts of param bounds\n",
    "              e.g., {\"RBF\": {\"lengthscale\": (0.1, 5.0)}}\n",
    "    \"\"\"\n",
    "    for name, sub_kernel in kernel.named_children():\n",
    "        # Recursively handle composite kernels (SumKernel, ProductKernel, etc.)\n",
    "        randomize_kernel_hyperparameters(sub_kernel, bounds)\n",
    "\n",
    "    kernel_type = type(kernel).__name__\n",
    "    param_bounds = bounds.get(kernel_type, {})\n",
    "\n",
    "    for param_name, param in kernel.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "\n",
    "        # Determine bounds\n",
    "        lower, upper = param_bounds.get(param_name, (0.1, 1.0))  # default bounds\n",
    "        shape = param.shape\n",
    "\n",
    "        # Sample new values\n",
    "        new_values = torch.tensor(\n",
    "            np.random.uniform(lower, upper, size=shape),\n",
    "            dtype=param.dtype\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            param.copy_(new_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.kernels import RBFKernel, ScaleKernel, MaternKernel\n",
    "from gpytorch.kernels import AdditiveKernel\n",
    "\n",
    "# Example composite kernel\n",
    "kernel = ScaleKernel(RBFKernel() + MaternKernel())\n",
    "\n",
    "bounds = {\n",
    "    \"RBFKernel\": {\n",
    "        \"lengthscale\": (0.1, 3.0)\n",
    "    },\n",
    "    \"WhiteNoiseKernel\": {\n",
    "        \"variance\": (1e-4, 1e-2)\n",
    "    },\n",
    "    \"ScaleKernel\": {\n",
    "        \"outputscale\": (0.5, 2.0)\n",
    "    }\n",
    "}\n",
    "\n",
    "randomize_kernel_hyperparameters(model.covar_module, bounds)\n",
    "list(model.named_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def sample_value(shape, bounds, dist_type):\n",
    "    \"\"\"Sample values from a given distribution type.\"\"\"\n",
    "    if dist_type == \"log-uniform\":\n",
    "        log_bounds = np.log(bounds)\n",
    "        return torch.tensor(\n",
    "            np.exp(np.random.uniform(*log_bounds, size=shape)),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "    elif dist_type == \"uniform\":\n",
    "        return torch.tensor(\n",
    "            np.random.uniform(*bounds, size=shape),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution type: {dist_type}\")\n",
    "\n",
    "def randomize_model_hyperparameters(\n",
    "    model,\n",
    "    param_specs,\n",
    "    default_bounds=(0.1, 1.0),\n",
    "    default_type=\"uniform\",\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomly reinitialize model hyperparameters using specified or fallback distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - model: GP model with .named_hyperparameters()\n",
    "    - param_specs: dict mapping param names to:\n",
    "          {\n",
    "              \"bounds\": (lower, upper),\n",
    "              \"type\": optional, overrides default_type\n",
    "          }\n",
    "    - default_bounds: fallback bounds if param not in param_specs\n",
    "    - default_type: fallback sampling distribution (\"uniform\" or \"log-uniform\")\n",
    "    - verbose: whether to print changes\n",
    "    \"\"\"\n",
    "\n",
    "    for name, param in model.named_hyperparameters():\n",
    "        spec = param_specs.get(name, {})\n",
    "\n",
    "        bounds = spec.get(\"bounds\", default_bounds)\n",
    "        dist_type = spec.get(\"type\", default_type)\n",
    "\n",
    "        shape = param.shape\n",
    "        new_value = sample_value(shape, bounds, dist_type)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            param.copy_(new_value)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Reinit] {name} ← {new_value.cpu().numpy()} (dist: {dist_type}, bounds: {bounds})\")\n",
    "\n",
    "\n",
    "\n",
    "param_specs = {\n",
    "    \"likelihood.noise\": {\"bounds\": (1e-4, 1e-1)},\n",
    "    \"covar_module.outputscale\": {\"bounds\": (0.5, 2.0)}\n",
    "}\n",
    "\n",
    "param_specs = {\n",
    "    \"likelihood.noise\": {\"bounds\": (1e-4, 1e-1)},  # log-uniform by default\n",
    "    \"covar_module.outputscale\": {\n",
    "        \"bounds\": (0.5, 2.0),\n",
    "        \"type\": \"uniform\"  # override to linear\n",
    "    },\n",
    "    \"model_parameters.signal_variance_2\": {\n",
    "        \"bounds\": (0.5, 2.0),\n",
    "        \"type\": \"uniform\"  # override to linear\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "randomize_model_hyperparameters(\n",
    "    model,\n",
    "    param_specs,\n",
    "    default_bounds=(0.1, 5.0),\n",
    "    default_type=\"log-uniform\",\n",
    "    verbose=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 10.5",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
