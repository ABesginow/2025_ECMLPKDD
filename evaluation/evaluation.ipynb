{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff3570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import gpytorch\n",
    "from helpers import util_functions, plotting_functions\n",
    "import itertools\n",
    "from lodegp import LODEGP\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb41dd6",
   "metadata": {},
   "source": [
    "#### Notes on file structure\n",
    "\n",
    "- results are stored in ../results/results\n",
    "- figures are stored in ../results/figures\n",
    "- folder structure is based on data (i.e. start/end point, number of datapoints and noise level)\n",
    "- file names are based on the actual model i.e. system name, l1 and l2 values \n",
    "- file names are separated by \"_\"\n",
    "- relevant results files are ...\n",
    "    - f\"{filename_addendum}_MLL.pkl\"\n",
    "    - f\"{filename_addendum}_MLL_logs.pkl\"\n",
    "    - f\"{filename_addendum}_MAP.pkl\"\n",
    "    - f\"{filename_addendum}_MAP_logs.pkl\"\n",
    "    - f\"{filename_addendum}_mean_ode_satisfaction_error_MLL.pkl\"\n",
    "    - f\"{filename_addendum}_mean_ode_satisfaction_error_MAP.pkl\"\n",
    "    - f\"{filename_addendum}_sample_ode_satisfaction_error_MLL.pkl\"\n",
    "    - f\"{filename_addendum}_sample_ode_satisfaction_error_MAP.pkl\"\n",
    "    - f\"{filename_addendum}_MLL_model_train_MSEs.pkl\"\n",
    "    - f\"{filename_addendum}_MAP_model_train_MSEs.pkl\"\n",
    "    - f\"{filename_addendum}_MLL_model_test_MSEs.pkl\"\n",
    "    - f\"{filename_addendum}_MAP_model_test_MSEs.pkl\"\n",
    "- relevant figure files are ...\n",
    "    - f\"MLL_model_posterior_{system_name}_l1-{l1_param_val}_l2-{l2_param_val}_{START}-{END}-{COUNT}_{noise_level}.png\"\n",
    "    - f\"MAP_model_posterior_{system_name}_l1-{l1_param_val}_l2-{l2_param_val}_{START}-{END}-{COUNT}_{noise_level}.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f61caa",
   "metadata": {},
   "source": [
    "#### Notes on relevant metrics / summaries / statistics\n",
    "- Visualize the trends for all metrics over increasing training data and noise\n",
    "    - Maybe plot number of datapoints on X and noise level on Y and color the points for MSE/MLL/MAP/... ? (This only works if working with a single model, as we have overlap otherwise)\n",
    "- Display some instances of good/bad trainings for both a lot and few datapoints\n",
    "- Show the difference that we get when we have few datapoints at the start and across the whole domain\n",
    "- Lineplot with different colors for different models, X is number of datapoints, Y is MSE/MLL/MAP/...\n",
    "    - When comparing the ODE satisfaction use a log-scale\n",
    "- Look at some interesting individual cases of the data\n",
    "    - Highlight the slow speed of the moon system in changing its values\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load a specific result from a model\n",
    "def load_model_result(model_name, result_name, results_path=None):\n",
    "    if results_path is None:\n",
    "        results_path = Path.cwd()\n",
    "        results_path = results_path.joinpath('results').joinpath(\"results\")\n",
    "    # Construct the file path based on the model and result names\n",
    "    file_path = results_path.joinpath(f'{model_name}_{result_name}.pkl')\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Result file '{file_path}' does not exist.\")\n",
    "    \n",
    "    # Load the result using dill\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = dill.load(file)\n",
    "    \n",
    "    return result\n",
    "\n",
    "load_model_result(\"Bipendulum_l1-1.0_l2-2.0\", \"MLL\", results_path=Path.cwd().parent.joinpath(\"results_bak\").joinpath(\"results\").joinpath(\"2-3-1_0.0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model_name(model_name, l1, l2):\n",
    "    \"\"\"\n",
    "    Constructs a model name based on the provided parameters.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The base name of the model.\n",
    "        l1 (float): The first parameter for the model.\n",
    "        l2 (float): The second parameter for the model.\n",
    "    \n",
    "    Returns:\n",
    "        str: The constructed model name.\n",
    "    \"\"\"\n",
    "    return f\"{model_name}_l1-{l1}_l2-{l2}\"\n",
    "\n",
    "\n",
    "def construct_experiment_name(start, end, count, noise):\n",
    "    return f\"{start}-{end}-{count}_{noise}\"\n",
    "\n",
    "\n",
    "def construct_experiment_path(start, end, count, noise, results_path=None):\n",
    "    \"\"\"\n",
    "    Constructs the path for the experiment results based on the provided parameters.\n",
    "    \n",
    "    Args:\n",
    "        start (float): The start value for the experiment.\n",
    "        end (float): The end value for the experiment.\n",
    "        count (int): The number of samples in the experiment.\n",
    "        noise (float): The noise level in the experiment.\n",
    "        result_path (str, optional): The base path for results. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        str: The constructed path for the experiment results.\n",
    "    \"\"\"\n",
    "    if results_path is None:\n",
    "        results_path = Path.cwd()\n",
    "        results_path = results_path.joinpath('results').joinpath(\"results\")\n",
    "    return results_path.joinpath(f\"{start}-{end}-{count}_{noise}\")\n",
    "\n",
    "\n",
    "print(construct_experiment_path(2, 3, 100, 0.1, results_path=Path.cwd().parent.joinpath(\"results\").joinpath(\"results\")).joinpath(construct_model_name(\"Bipendulum\", 1.0, 2.0)))\n",
    "print(construct_experiment_path(2, 3, 100, 0.1, results_path=Path.cwd().parent.joinpath(\"results\").joinpath(\"results\")).exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy Metric Call Dict\n",
    "emcd = {\"avg neg MLL\" : \"MLL\",\n",
    "\"MLL logs\": \"MLL_logs\",\n",
    "\"avg neg MAP\" : \"MAP\",\n",
    "\"MAP logs\": \"MAP_logs\",\n",
    "\"mean MLL ODE error\": \"mean_ode_satisfaction_error_MLL\",\n",
    "\"mean MAP ODE error\": \"mean_ode_satisfaction_error_MAP\",\n",
    "\"sample MLL ODE error\": \"sample_ode_satisfaction_error_MLL\",\n",
    "\"sample MAP ODE error\": \"sample_ode_satisfaction_error_MAP\",\n",
    "\"MLL train MSEs\": \"MLL_model_train_MSEs\",\n",
    "\"MAP train MSEs\": \"MAP_model_train_MSEs\",\n",
    "\"MLL test MSEs\": \"MLL_model_test_MSEs\",\n",
    "\"MAP test MSEs\": \"MAP_model_test_MSEs\",\n",
    "\"MLL state dict\": \"MLL_state_dict\",\n",
    "\"MAP state dict\": \"MAP_state_dict\"}\n",
    "\n",
    "# More used as a reference than for actual use\n",
    "all_model_names = [\"Bipendulum\", \"Bipendulum first equation\", \"Bipendulum second equation\", \"Bipendulum Sum eq2 diffed\", \"Bipendulum moon gravitation\", \"Bipendulum Parameterized\", \"No system\"]\n",
    "\n",
    "all_l1_l2_combinations = list([[1.0, 2.0], [1.0, 3.0], [2.0, 3.0]])\n",
    "\n",
    "all_model_settings = list(itertools.chain(itertools.product([\"Bipendulum\", \"Bipendulum first equation\", \"Bipendulum second equation\", \"Bipendulum Sum eq2 diffed\", \"Bipendulum moon gravitation\"], [[1.0, 2.0], [1.0, 3.0], [2.0, 3.0]]), [(\"Bipendulum Parameterized\", [1.0, 2.0]),  (\"No system\", [1.0, 2.0])]))\n",
    "\n",
    "all_ranges = [(2, 12), (2, 3)]\n",
    "all_dataset_sizes = [2, 5, 10, 20, 50, 100]\n",
    "all_noises = [0.0, 0.1, 0.2, 0.3]\n",
    "\n",
    "all_experiment_settings = list(itertools.product([(2, 12), (2, 3)], [1, 2, 5, 10, 20, 50, 100], [0.0, 0.1, 0.2, 0.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define line style settings for the various model settings\n",
    "# Each model name gets their own color\n",
    "# Each l1-l2 combination gets its own line style\n",
    "model_colors = {\n",
    "    \"Bipendulum\":  \"blue\",\n",
    "    \"Bipendulum first equation\":  \"orange\",\n",
    "    \"Bipendulum second equation\":  \"green\",\n",
    "    \"Bipendulum Sum eq2 diffed\":  \"red\",\n",
    "    \"Bipendulum moon gravitation\":  \"purple\",\n",
    "    \"Bipendulum Parameterized\":  \"brown\",\n",
    "    \"No system\":  \"pink\"\n",
    "}\n",
    "# Define line styles for the l1-l2 combinations\n",
    "line_styles = {\n",
    "    (1.0, 2.0):  \"-\",\n",
    "    (1.0, 3.0):  \"--\",\n",
    "    (2.0, 3.0):  \"dotted\"\n",
    "}\n",
    "def get_line_plot_settings(model_name, l1, l2):\n",
    "    \"\"\"\n",
    "    Get the line plot settings for a given model name and l1-l2 combination.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The name of the model.\n",
    "        l1 (float): The first parameter for the model.\n",
    "        l2 (float): The second parameter for the model.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the color and linestyle for the plot.\n",
    "    \"\"\"\n",
    "    return model_colors[model_name], line_styles[(l1, l2)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f4fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unpack_experiment_setting(experiment_setting):\n",
    "    return *experiment_setting[0], *experiment_setting[1:]\n",
    "\n",
    "def unpack_model_setting(model_setting):\n",
    "    return model_setting[0], *model_setting[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58054d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results_path = Path.cwd().parent.joinpath(\"results_bak\").joinpath(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_result(f\"{construct_model_name(all_model_settings[0][0], *all_model_settings[0][1])}\", emcd['avg neg MLL'],  construct_experiment_path(*all_experiment_settings[0][0], *all_experiment_settings[0][1:], results_path=base_results_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3470b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_result(f\"{construct_model_name(*unpack_model_setting(all_model_settings[0]))}\", emcd['avg neg MLL'],  construct_experiment_path(*unpack_experiment_setting(all_experiment_settings[0]), results_path=base_results_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_setting, model_setting in itertools.product(all_experiment_settings, all_model_settings):\n",
    "    try:\n",
    "        model_name = construct_model_name(*unpack_model_setting(model_setting))\n",
    "        experiment_path = construct_experiment_path(*unpack_experiment_setting(experiment_setting), results_path=base_results_path)\n",
    "        result = load_model_result(f\"{model_name}\", emcd['avg neg MLL'], experiment_path)\n",
    "        print(f\"Successfully loaded result for {model_setting} in experiment {experiment_setting}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading result for {model_setting} in experiment {experiment_setting}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data_generating_fkt_ode_error(data):\n",
    "    train_x = torch.linspace(0, 1, 1)\n",
    "    train_y = torch.linspace(0, 1, 1) \n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks = 3)\n",
    "    model = LODEGP.LODEGP(train_x, train_y, likelihood, num_tasks=3, ODE_name=\"Bipendulum\", l1=1.0, l2=2.0)\n",
    "\n",
    "    y0_func = lambda x: float(781/8000)*torch.sin(x)/x - float(1/20)*torch.cos(x)/x**2 + float(1/20)*torch.sin(x)/x**3\n",
    "    y1_func = lambda x: float(881/8000)*torch.sin(x)/x - float(1/40)*torch.cos(x)/x**2 + float(1/40)*torch.sin(x)/x**3\n",
    "    y2_func = lambda x: float(688061/800000)*torch.sin(x)/x - float(2543/4000)*torch.cos(x)/x**2 + float(1743/4000)*torch.sin(x)/x**3 - float(3/5)*torch.cos(x)/x**4 + float(3/5)*torch.sin(x)/x**5 \n",
    "    y_func = lambda x: torch.stack([y0_func(x), y1_func(x), y2_func(x)], dim=-1)\n",
    "    for i, row in enumerate(model.A):\n",
    "        error = util_functions.calculate_differential_equation_error_numeric(row, model.sage_locals, y_func, data)\n",
    "        error = torch.sum(error)\n",
    "        print(f\"Error in row {i}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80887dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_data_generating_fkt_ode_error(torch.linspace(2, 3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25838547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_results_for_metric(target_metric, experiment_setting_filter=None, model_setting_filter=None):\n",
    "\n",
    "    all_results = {}#{str(experiment_setting)+str(model_setting) : None for experiment_setting, model_setting in itertools.product(all_experiment_settings, all_model_settings)}\n",
    "\n",
    "    for experiment_setting, model_setting in itertools.product(all_experiment_settings, all_model_settings):\n",
    "        if experiment_setting_filter:\n",
    "            if not experiment_setting_filter(experiment_setting):\n",
    "                continue\n",
    "        if model_setting_filter:\n",
    "            if not model_setting_filter(model_setting):\n",
    "                continue\n",
    "        model_name = construct_model_name(*unpack_model_setting(model_setting))\n",
    "        experiment_path = construct_experiment_path(*unpack_experiment_setting(experiment_setting), results_path=base_results_path)\n",
    "        result = load_model_result(f\"{model_name}\", target_metric, experiment_path)\n",
    "        all_results[str(experiment_setting) + str(model_setting)] = result\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c87b02",
   "metadata": {},
   "source": [
    "# How does MAP behave over increasing dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 1, figsize=(10, 20))\n",
    "for i, noise in enumerate(all_noises):\n",
    "    max_map = [-np.inf] * len(all_dataset_sizes)\n",
    "    min_map = [np.inf] * len(all_dataset_sizes)\n",
    "    for model_setting in all_model_settings:\n",
    "        #if model_setting[1][0] != 1.0 or model_setting[1][1] != 2.0:\n",
    "        #    continue\n",
    "        all_neg_maps = []\n",
    "        for dataset_size in all_dataset_sizes:\n",
    "            target_metric = emcd['avg neg MAP']\n",
    "            # [(2, 12), (2, 3)], [1, 2, 5, 10, 20, 50, 100], [0.0, 0.1, 0.2, 0.3]\n",
    "            experiment_setting_filter = lambda x: x[0] == (2, 3) and x[1] == dataset_size and x[2] == noise \n",
    "            model_setting_filter = lambda x: x[0] == model_setting[0] and x[1] == model_setting[1]\n",
    "            cur_neg_map = list(get_all_results_for_metric(target_metric, experiment_setting_filter, model_setting_filter).values())[0].item()\n",
    "            all_neg_maps.append(cur_neg_map)\n",
    "            if cur_neg_map < min_map[all_dataset_sizes.index(dataset_size)]:\n",
    "                min_map[all_dataset_sizes.index(dataset_size)] = all_neg_maps[-1]\n",
    "            if cur_neg_map > max_map[all_dataset_sizes.index(dataset_size)]:\n",
    "                max_map[all_dataset_sizes.index(dataset_size)] = all_neg_maps[-1]\n",
    "        \n",
    "        color, linestyle =  get_line_plot_settings(model_setting[0], model_setting[1][0], model_setting[1][1])\n",
    "        axs[2*i].plot([d for d in all_dataset_sizes if d < 1000], all_neg_maps, label=f\"{model_setting[0]} l1-{model_setting[1][0]} l2-{model_setting[1][1]}\", color=color, linestyle=linestyle)\n",
    "        axs[2*i].set_xlabel(\"Dataset Size\")\n",
    "        axs[2*i].set_ylabel(\"Average Negative MAP\")\n",
    "        axs[2*i].set_title(f\"Average Negative MAP for {noise}. Smaller=Better\")\n",
    "        axs[2*i].grid()\n",
    "        axs[2*i].legend(bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "    axs[2*i+1].plot(all_dataset_sizes, np.array(min_map) - np.array(max_map))\n",
    "    axs[2*i+1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d4af30",
   "metadata": {},
   "source": [
    "# Train/Test MSE over dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6392ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = \"test\" # \"train\" or \"test\"\n",
    "\n",
    "fig, axs = plt.subplots(8, 1, figsize=(10, 28))\n",
    "for i, noise in enumerate(all_noises):\n",
    "    max_mse = [-np.inf] * len(all_dataset_sizes)\n",
    "    min_mse = [np.inf] * len(all_dataset_sizes)\n",
    "    for model_setting in all_model_settings:\n",
    "        #if model_setting[1][0] != 1.0 or model_setting[1][1] != 2.0:\n",
    "        #    continue\n",
    "        all_avg_MSEs = []\n",
    "        for dataset_size in all_dataset_sizes:\n",
    "            target_metric = emcd[f'MAP {train_test} MSEs']\n",
    "            # [(2, 12), (2, 3)], [1, 2, 5, 10, 20, 50, 100], [0.0, 0.1, 0.2, 0.3]\n",
    "            experiment_setting_filter = lambda x: x[0] == (2, 12) and x[1] == dataset_size and x[2] == noise \n",
    "            model_setting_filter = lambda x: x[0] == model_setting[0] and x[1] == model_setting[1]\n",
    "            cur_MSE = list(get_all_results_for_metric(target_metric, experiment_setting_filter, model_setting_filter).values())[0]\n",
    "            cur_avg_MSE = np.log(np.mean(cur_MSE))\n",
    "            all_avg_MSEs.append(cur_avg_MSE)\n",
    "            if cur_avg_MSE < min_mse[all_dataset_sizes.index(dataset_size)]:\n",
    "                min_mse[all_dataset_sizes.index(dataset_size)] = all_avg_MSEs[-1]\n",
    "            if cur_avg_MSE > max_mse[all_dataset_sizes.index(dataset_size)]:\n",
    "                max_mse[all_dataset_sizes.index(dataset_size)] = all_avg_MSEs[-1]\n",
    "        color, linestyle =  get_line_plot_settings(model_setting[0], model_setting[1][0], model_setting[1][1])\n",
    "        axs[2*i].plot([d for d in all_dataset_sizes if d < 1000], all_avg_MSEs, label=f\"{model_setting[0]} l1-{model_setting[1][0]} l2-{model_setting[1][1]}\", color=color, linestyle=linestyle)\n",
    "        axs[2*i].set_xlabel(\"Dataset Size\")\n",
    "        axs[2*i].set_ylabel(\"log avg. {train_test} MSE on MAP trained model\")\n",
    "        axs[2*i].set_title(f\"log avg. {train_test} MSE on MAP trained model for noise {noise}. Smaller=Better\")\n",
    "        axs[2*i].legend(bbox_to_anchor=(1.05, 1))\n",
    "\n",
    "    axs[2*i+1].plot(all_dataset_sizes, np.array(min_mse) - np.array(max_mse))\n",
    "    axs[2*i+1].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e43cdb9",
   "metadata": {},
   "source": [
    "## Tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for dataset_size in all_dataset_sizes:\n",
    "    print(f\"Dataset size {dataset_size}:\\\\\\\\\")\n",
    "    tabular_columns = [\"Model name\", \"log MSE 0.0\", \"log MSE 0.1\", \"log MSE 0.2\", \"log MSE 0.3\"]\n",
    "    tabular_rows = []\n",
    "    min_error = [np.inf]*len(all_noises)\n",
    "    for model_setting in all_model_settings:\n",
    "        tabular_row = []\n",
    "        tabular_row.append(construct_model_name(*unpack_model_setting(model_setting)).replace(\"_\", \" \"))\n",
    "        for i, noise in enumerate(all_noises):\n",
    "            all_avg_MSEs = []\n",
    "            train_test = \"train\" # \"train\" or \"test\"\n",
    "            target_metric = emcd[f'MAP {train_test} MSEs']\n",
    "\n",
    "\n",
    "            # [(2, 12), (2, 3)], [1, 2, 5, 10, 20, 50, 100], [0.0, 0.1, 0.2, 0.3]\n",
    "            experiment_setting_filter = lambda x: x[0] == (2, 3) and x[1] == dataset_size and x[2] == noise \n",
    "            model_setting_filter = lambda x: x[0] == model_setting[0] and x[1] == model_setting[1]\n",
    "            cur_MSE = list(get_all_results_for_metric(target_metric, experiment_setting_filter, model_setting_filter).values())[0]\n",
    "            cur_avg_MSE = np.round(np.log(np.mean(cur_MSE)), 3)\n",
    "            all_avg_MSEs.append(cur_avg_MSE)\n",
    "            tabular_row.append(cur_avg_MSE)\n",
    "            if cur_avg_MSE < min_error[i]:\n",
    "                min_error[i] = all_avg_MSEs\n",
    "\n",
    "        tabular_rows.append(tabular_row)\n",
    "\n",
    "    # Print as latex table\n",
    "    print(\"\\\\begin{tabular}{|c|\" + \"|\".join([\"c\"] * (len(tabular_columns) - 1)) + \"|}\")\n",
    "    print(\"\\\\hline\")\n",
    "    print(\" & \".join(tabular_columns) + \" \\\\\\\\\")\n",
    "    print(\"\\\\hline\")\n",
    "    for row in tabular_rows:\n",
    "        print(\" & \".join([str(r) if not r == min_error[ind-1] else f\"\\\\textbf{{{r}}}\" for ind, r in enumerate(row)]) + \" \\\\\\\\\")\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"\\\\end{tabular}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00bcb7c",
   "metadata": {},
   "source": [
    "# The trained noise levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(4, 3, figsize=(18, 20))\n",
    "for i, noise in enumerate(all_noises):\n",
    "    START = 2\n",
    "    END = 12 \n",
    "\n",
    "    y0_func = lambda x: float(781/8000)*torch.sin(x)/x - float(1/20)*torch.cos(x)/x**2 + float(1/20)*torch.sin(x)/x**3\n",
    "    y1_func = lambda x: float(881/8000)*torch.sin(x)/x - float(1/40)*torch.cos(x)/x**2 + float(1/40)*torch.sin(x)/x**3\n",
    "    y2_func = lambda x: float(688061/800000)*torch.sin(x)/x - float(2543/4000)*torch.cos(x)/x**2 + float(1743/4000)*torch.sin(x)/x**3 - float(3/5)*torch.cos(x)/x**4 + float(3/5)*torch.sin(x)/x**5 \n",
    "    y0_noise_level = (torch.max(y0_func(torch.linspace(START, END, 100)))*noise)\n",
    "    y1_noise_level = (torch.max(y1_func(torch.linspace(START, END, 100)))*noise)\n",
    "    y2_noise_level = (torch.max(y2_func(torch.linspace(START, END, 100)))*noise)\n",
    "\n",
    "    gt_noise_levels = torch.tensor([y0_noise_level, y1_noise_level, y2_noise_level])\n",
    "    print(f\"{noise} noise levels: {gt_noise_levels.tolist()}\")\n",
    "    for channel in range(3):\n",
    "        gt_noise = gt_noise_levels[channel].item()\n",
    "        # Make a thick dashed red line for the ground truth noise level\n",
    "        axs[i, channel].axhline(y=np.log(gt_noise), color='red', linestyle='--', linewidth=2, label='Ground Truth Noise Level')\n",
    "    max_mll = [-np.inf] * len(all_dataset_sizes)\n",
    "    min_mll = [np.inf] * len(all_dataset_sizes)\n",
    "    for model_setting in all_model_settings:\n",
    "        #if model_setting[1][0] != 1.0 or model_setting[1][1] != 2.0:\n",
    "        #    continue\n",
    "        all_noise_levels = []\n",
    "        all_sqrt_noise_levels = []\n",
    "        all_diffs_to_gt = []\n",
    "        for dataset_size in all_dataset_sizes:\n",
    "\n",
    "            target_metric = emcd['MAP state dict']\n",
    "            # [(2, 12), (2, 3)], [1, 2, 5, 10, 20, 50, 100], [0.0, 0.1, 0.2, 0.3]\n",
    "            experiment_setting_filter = lambda x: x[0] == (START, END) and x[1] == dataset_size and x[2] == noise \n",
    "            model_setting_filter = lambda x: x[0] == model_setting[0] and x[1] == model_setting[1]\n",
    "            cur_state_dict = list(get_all_results_for_metric(target_metric, experiment_setting_filter, model_setting_filter).values())[0]\n",
    "\n",
    "            cur_task_noises = cur_state_dict['likelihood.raw_task_noises']\n",
    "            cur_global_noise = cur_state_dict['likelihood.raw_noise']\n",
    "            # The sum of softplussed noises is the total noise for each channel\n",
    "            cur_total_noises = torch.nn.functional.softplus(cur_task_noises) + torch.nn.functional.softplus(cur_global_noise)\n",
    "            all_noise_levels.append(cur_total_noises)\n",
    "            cur_total_noises_sqrt = torch.sqrt(cur_total_noises)\n",
    "            all_sqrt_noise_levels.append(cur_total_noises_sqrt)\n",
    "\n",
    "            diff_to_gt = torch.abs(cur_total_noises_sqrt - gt_noise_levels)\n",
    "            all_diffs_to_gt.append(diff_to_gt)\n",
    "\n",
    "        for channel in range(3):\n",
    "            #axs[i, channel].plot(all_dataset_sizes, [np.log(all_diffs_to_gt[j][channel].item()) for j in range(len(all_dataset_sizes))], label=f\"{model_setting[0]} l1-{model_setting[1][0]} l2-{model_setting[1][1]} channel {channel}\")\n",
    "            #axs[i, channel].set_title(f\"log trained noise on MLL trained model for noise {noise} channel {channel}. Smaller=Better\")\n",
    "\n",
    "            color, linestyle =  get_line_plot_settings(model_setting[0], model_setting[1][0], model_setting[1][1])\n",
    "            axs[i, channel].plot(all_dataset_sizes, [np.log(all_sqrt_noise_levels[j][channel].item()) for j in range(len(all_dataset_sizes))], label=f\"{model_setting[0]} l1-{model_setting[1][0]} l2-{model_setting[1][1]} channel {channel}\", color=color, linestyle=linestyle)\n",
    "            axs[i, channel].set_title(f\"log trained noise on MAP trained model for noise {noise} channel {channel}. Closer to ground truth=Better\")\n",
    "            axs[i, channel].set_xlabel(\"Dataset Size\")\n",
    "            axs[i, channel].set_ylabel(\"log trained noise on MAP trained model\")\n",
    "            #axs[i, channel].legend(bbox_to_anchor=(1.05, 1))\n",
    "            axs[i, channel].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d09209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e332b74",
   "metadata": {},
   "source": [
    "# ODE satisfaction\n",
    "Note: ODE satisfaction is calculated on test_x i.e. with `torch.linspace(1e-3, 15, 100)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d83fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 1, figsize=(10, 20))\n",
    "for i, noise in enumerate(all_noises):\n",
    "    for model_setting in all_model_settings:\n",
    "        #if (model_setting[1][0] != 1.0 or model_setting[1][1] != 2.0):\n",
    "        #    continue\n",
    "        all_avg_MSEs = []\n",
    "        for dataset_size in all_dataset_sizes:\n",
    "            target_metric = emcd['mean MAP ODE error']\n",
    "            # [(2, 12), (2, 3)], [1, 2, 5, 10, 20, 50, 100], [0.0, 0.1, 0.2, 0.3]\n",
    "            experiment_setting_filter = lambda x: x[0] == (2, 3) and x[1] == dataset_size and x[2] == noise \n",
    "            model_setting_filter = lambda x: x[0] == model_setting[0] and x[1] == model_setting[1]\n",
    "            cur_MSE = list(get_all_results_for_metric(target_metric, experiment_setting_filter, model_setting_filter).values())[0]\n",
    "            cur_avg_MSE = torch.log(torch.mean(torch.abs(cur_MSE)))\n",
    "            all_avg_MSEs.append(cur_avg_MSE.detach().numpy())\n",
    "\n",
    "        color, linestyle =  get_line_plot_settings(model_setting[0], model_setting[1][0], model_setting[1][1])\n",
    "        axs[i].plot([d for d in all_dataset_sizes if d < 1000], all_avg_MSEs, label=f\"{model_setting[0]} l1-{model_setting[1][0]} l2-{model_setting[1][1]}\", color=color, linestyle=linestyle)\n",
    "        axs[i].set_xlabel(\"Dataset Size\")\n",
    "        axs[i].set_ylabel(\"mean ODE error on MAP trained model\")\n",
    "        axs[i].set_title(f\"mean ODE error on MAP trained model for noise {noise}. Smaller=Better\")\n",
    "        axs[i].legend(bbox_to_anchor=(1.05, 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage3_129",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
