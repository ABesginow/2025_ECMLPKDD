{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6017a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finite_difference import central_difference\n",
    "import gpytorch\n",
    "from LODEGP.LODEGP import LODEGP, list_standard_models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8ef932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, Y, return_figure=False, title_add=\"\", fig=None, ax=None, display_figure=True):\n",
    "    if not (fig and ax):\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        ax = fig.add_subplot(111)\n",
    "    ax.plot(X.numpy(), Y.numpy(), 'k.')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title(f\"Data {title_add}\")\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return fig, ax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_model(model, likelihood, X, Y, display_figure=True, return_figure=False, figure=None,\n",
    "               ax=None, loss_val=None, loss_type = None):\n",
    "    interval_length = torch.max(X) - torch.min(X)\n",
    "    shift = interval_length * 0.0 \n",
    "    test_x = torch.linspace(torch.min(\n",
    "        X) - shift, torch.max(X) + shift, 1000)\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if not (figure and ax):\n",
    "            figure, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        ax.plot(X.numpy(), Y.numpy(), 'k.', zorder=2)\n",
    "        ax.plot(test_x.numpy(), observed_pred.mean.numpy(), color=\"b\", zorder=3)\n",
    "        amount_of_gradient_steps = 30\n",
    "        alpha_min = 0.05\n",
    "        alpha_max = 0.8\n",
    "        alpha = (alpha_max-alpha_min)/amount_of_gradient_steps\n",
    "        c = ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(\n",
    "        ), alpha=alpha+alpha_min, zorder=1).get_facecolor()\n",
    "\n",
    "        for i in range(1, amount_of_gradient_steps):\n",
    "            ax.fill_between(test_x.numpy(), (lower+(i/amount_of_gradient_steps)*(upper-lower)).numpy(),\n",
    "                            (upper-(i/amount_of_gradient_steps)*(upper-lower)).numpy(), alpha=alpha, color=c, zorder=1)\n",
    "        ax.plot([], [], 'k.', label=\"Data\")\n",
    "        ax.plot([], [], 'b', label=\"Mean\")\n",
    "        ax.plot([], [], color=c, alpha=1.0, label=\"Confidence\")\n",
    "        ax.legend(loc=\"upper left\")\n",
    "        ax.set_xlabel(\"Normalized Input\")\n",
    "        ax.set_ylabel(\"Normalized Output\")\n",
    "        ax.set_title(f\"{loss_type}: {loss_val:.2f}\")\n",
    "    if not return_figure and display_figure:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return figure, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_gp_predictions(\n",
    "    X_train, Y_train, \n",
    "    X_test, Y_pred_mean, Y_pred_var, \n",
    "    n_std=2,\n",
    "    show=True, \n",
    "    return_fig=False,\n",
    "    fig=None, \n",
    "    ax=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot predictions from a single- or multi-output Gaussian Process model.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: (N,) training inputs\n",
    "    - Y_train: (N,) or (N, D) training outputs\n",
    "    - X_test: (M,) test inputs\n",
    "    - Y_pred_mean: (M,) or (M, D) predicted mean at test inputs\n",
    "    - Y_pred_var: (M,) or (M, D) predicted variance at test inputs\n",
    "    - n_std: number of standard deviations for credible interval (default: 2)\n",
    "    - show: whether to call plt.show() (default: True)\n",
    "    - return_fig: whether to return fig object (default: False)\n",
    "    - fig, ax: optionally provide existing matplotlib Figure and Axes (for subplots)\n",
    "    \n",
    "    Returns:\n",
    "    - fig (optional): the matplotlib Figure object if return_fig is True\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure X inputs are 1D arrays\n",
    "    X_train = np.asarray(X_train).squeeze()\n",
    "    X_test = np.asarray(X_test).squeeze()\n",
    "\n",
    "    # Convert outputs to 2D only if needed\n",
    "    Y_train = np.asarray(Y_train)\n",
    "    Y_pred_mean = np.asarray(Y_pred_mean)\n",
    "    Y_pred_var = np.asarray(Y_pred_var)\n",
    "\n",
    "    if Y_train.ndim == 1:\n",
    "        Y_train = Y_train[:, np.newaxis]\n",
    "    if Y_pred_mean.ndim == 1:\n",
    "        Y_pred_mean = Y_pred_mean[:, np.newaxis]\n",
    "    if Y_pred_var.ndim == 1:\n",
    "        Y_pred_var = Y_pred_var[:, np.newaxis]\n",
    "\n",
    "    num_outputs = Y_train.shape[1]\n",
    "\n",
    "    if ax is None or fig is None:\n",
    "        fig, ax = plt.subplots(num_outputs, 1, figsize=(8, 3 * num_outputs), squeeze=False)\n",
    "\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for i in range(num_outputs):\n",
    "        ax_i = ax[i]\n",
    "\n",
    "        # Training data\n",
    "        ax_i.scatter(X_train, Y_train[:, i], color='black', s=20, label='Training Data', zorder=3, marker='x')\n",
    "\n",
    "        # Posterior mean\n",
    "        ax_i.plot(X_test, Y_pred_mean[:, i], color='blue', label='Posterior Mean')\n",
    "\n",
    "        # Credible interval (shaded)\n",
    "        std_dev = np.sqrt(Y_pred_var[:, i])\n",
    "        lower = Y_pred_mean[:, i] - n_std * std_dev\n",
    "        upper = Y_pred_mean[:, i] + n_std * std_dev\n",
    "\n",
    "        ax_i.fill_between(\n",
    "            X_test, lower, upper,\n",
    "            color='blue', alpha=0.3, label=f'{n_std}$\\sigma$ Interval'\n",
    "        )\n",
    "\n",
    "        ax_i.set_title(f\"Output Dimension {i}\")\n",
    "        ax_i.set_xlabel(\"Input\")\n",
    "        ax_i.set_ylabel(\"Output\")\n",
    "        ax_i.legend()\n",
    "        ax_i.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    if return_fig:\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d759c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 2\n",
    "END = 12\n",
    "COUNT = 100\n",
    "train_x = torch.linspace(START, END, COUNT)\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=3)\n",
    "\n",
    "y0_func = lambda x: float(781/8000)*torch.sin(x)/x - float(1/20)*torch.cos(x)/x**2 + float(1/20)*torch.sin(x)/x**3\n",
    "y1_func = lambda x: float(881/8000)*torch.sin(x)/x - float(1/40)*torch.cos(x)/x**2 + float(1/40)*torch.sin(x)/x**3\n",
    "y2_func = lambda x: float(688061/800000)*torch.sin(x)/x - float(2543/4000)*torch.cos(x)/x**2 + float(1743/4000)*torch.sin(x)/x**3 - float(3/5)*torch.cos(x)/x**4 + float(3/5)*torch.sin(x)/x**5 \n",
    "y0 = y0_func(train_x)\n",
    "y1 = y1_func(train_x)\n",
    "y2 = y2_func(train_x)\n",
    "train_y = torch.stack([y0, y1, y2], dim=1)\n",
    "\n",
    "model = LODEGP(train_x, train_y, likelihood, 3, ODE_name=\"Bipendulum\", verbose=True, system_parameters={\"l1\": 1.0, \"l2\": 2.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af795fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior GP and the data\n",
    "model.eval()\n",
    "model.likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    test_x = torch.linspace(2, 12, 100)\n",
    "    observed_pred = model.likelihood(model(test_x))\n",
    "    observed_pred_mean = observed_pred.mean\n",
    "    observed_pred_var = observed_pred.covariance_matrix.diag().reshape(-1, 3)\n",
    "    plot_gp_predictions(train_x, train_y, test_x, observed_pred_mean, observed_pred_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage3_129",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
